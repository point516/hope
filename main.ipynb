{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query data from PostgreSQL table, save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\", dbname='cs', user='postgres', password='123456', port=5432)\n",
    "cur = conn.cursor()\n",
    "\n",
    "query = \"\"\"SELECT * FROM side_dataset where id>=684\"\"\"\n",
    "cur.execute(query)\n",
    "\n",
    "rows = cur.fetchall()\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "df.to_csv('df_642.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('df_642.csv')\n",
    "#First 4 rows are not needed\n",
    "df = data.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While parsing the data, I was storing this particular stat (average rounds lost(won) on a map) as 0's in case of missing data. So, I convert them to NULL, to deal with them further. Also deelting the rows with incorrect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['t1_rounds_lost']>0) & (df['t1_rounds_won'] == 0), 't1_rounds_won'] = None\n",
    "df.loc[(df['t1_rounds_lost']==0) & (df['t1_rounds_won'] > 0), 't1_rounds_lost'] = None\n",
    "df.loc[(df['t2_rounds_lost']==0) & (df['t2_rounds_won'] > 0), 't2_rounds_lost'] = None\n",
    "df.loc[(df['t2_rounds_lost']>0) & (df['t2_rounds_won'] == 0), 't2_rounds_won'] = None\n",
    "\n",
    "df.drop(df[(df['t2_5v4']==0) & (df['t2_played']>0)].index, axis=0, inplace=True)\n",
    "df.drop(df[(df['t1_5v4']==0) & (df['t1_played']>0)].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try filling NULL values with 13's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t1_rounds_lost'].fillna(13, inplace=True)\n",
    "df['t1_rounds_won'].fillna(13, inplace=True)\n",
    "df['t2_rounds_lost'].fillna(13, inplace=True)\n",
    "df['t2_rounds_won'].fillna(13, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1. The differences between team stats will be the predicting features, so creating the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_name = [df.columns[i]+'_dif' for i in range(0,len(df.columns[:22]),2)] + [df.columns[i]+'_dif' for i in range(22, 24)] + [df.columns[i]+'_dif' for i in range(25, 33)]\n",
    "\n",
    "dataset = pd.DataFrame(columns=cols_name)\n",
    "\n",
    "for index, name in enumerate(cols_name[:11]):\n",
    "    dataset[name] = df[df.columns[index*2]] - df[df.columns[index*2+1]]\n",
    "\n",
    "for index, name in enumerate(cols_name[11:13]):\n",
    "    dataset[name] = df[df.columns[22+index]] - df[df.columns[33+index]]\n",
    "\n",
    "for index, name in enumerate(cols_name[13:]):\n",
    "    dataset[name] = df[df.columns[25+index]] - df[df.columns[36+index]]\n",
    "\n",
    "dataset['t1_fp'] = df['t1_fp']\n",
    "dataset['t2_fp'] = df['t2_fp']\n",
    "dataset['result'] = df['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "columns = ['t1_winstreak_dif','t1_h2h_dif','t1_ranking_dif','t1_pluses_dif','t1_minuses_dif','t1_coef_dif','t1_rating_dif','t1_event_rating_dif', \\\n",
    "            't1_num_maps_dif','t1_avg_lost_dif','t1_avg_won_dif','t1_rounds_lost_dif','t1_rounds_won_dif','t1_fp_percent_dif','t1_winrate_dif', \\\n",
    "            't1_played_dif','t1_map_winstreak_dif','t1_map_losestreak_dif','t1_5v4_dif','t1_4v5_dif','t1_pistol_dif']\n",
    "\n",
    "scaler.fit(dataset.loc[:,columns])\n",
    "dataset.loc[:,columns] = scaler.transform(dataset.loc[:,columns])\n",
    "\n",
    "with open('scaler.pkl','wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "dataset.to_csv('just_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 23)\n",
      "(100, 23)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('just_scaled.csv')\n",
    "\n",
    "Y = dataset.loc[:,['result']]\n",
    "X = dataset.drop('result',axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=100, shuffle=True, random_state=42)\n",
    "# x_val, x_test, y_val, y_test = train_test_split(x_rest, y_rest, test_size=100, shuffle=True, random_state=42)\n",
    "print(x_train.shape)\n",
    "# print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf_name</th>\n",
       "      <th>clf_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.564442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.549057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.527903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clf_name  clf_accuracy\n",
       "1  CatBoost      0.564442\n",
       "2  LightGBM      0.549057\n",
       "0   XGBoost      0.527903"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults = pd.DataFrame([['XGBoost',0],['CatBoost',0],['LightGBM',0]],columns=['clf_name', 'clf_accuracy'])\n",
    "models = [XGBClassifier(random_state=random_state), CatBoostClassifier(random_state=random_state, verbose=0), LGBMClassifier(random_state=random_state)]\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    accuracies = []  # List to store the accuracy of each fold\n",
    "    for train_index, test_index in cv.split(x_train):\n",
    "        fx_train, fx_test = x_train.iloc[train_index], x_train.iloc[test_index]\n",
    "        fy_train, fy_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        model.fit(fx_train, fy_train.values.ravel())\n",
    "\n",
    "        preds = model.predict(fx_test)\n",
    "\n",
    "        if index == 1:\n",
    "            preds = [True if item=='True' else False for item in model.predict(fx_test)]\n",
    "\n",
    "        accuracies.append(accuracy_score(fy_test, preds))\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    defaults.loc[index, 'clf_accuracy'] = mean_accuracy\n",
    "\n",
    "defaults.sort_values(by='clf_accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 300, step=1),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 0, 20, step=1),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\n",
    "            \"colsample_bylevel\", 0.5, 1, step=0.1\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1, step=0.1),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0, 1, step=0.1),\n",
    "        #\"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\"]),\n",
    "        \"bagging_temperature\": trial.suggest_float('bagging_temperature', 0, 20, step=0.25)\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "                              **param_grid,\n",
    "                              iterations=10000,\n",
    "                              loss_function='Logloss',\n",
    "                              random_seed=42,\n",
    "                              early_stopping_rounds=50,\n",
    "                              eval_metric='Accuracy',\n",
    "                              verbose=0\n",
    "                              )\n",
    " \n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = [True if item=='True' else False for item in model.predict(X_test)]\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"CB Classifier\")\n",
    "func = lambda trial: cb_objective(trial, x_train, np.ravel(y_train.values))\n",
    "study.optimize(func, n_trials=100)\n",
    "\n",
    "print(f\"\\tBest value : {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value : 0.65317\n",
    "\tBest params:\n",
    "\t\tlearning_rate: 0.13803526386053072\n",
    "\t\tmax_depth: 4\n",
    "\t\tl2_leaf_reg: 19\n",
    "\t\tcolsample_bylevel: 1.0\n",
    "\t\tsubsample: 0.7\n",
    "\t\trandom_strength: 1.0\n",
    "\t\tbagging_temperature: 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"eta\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0, 10, step=1),\n",
    "        \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 0, 20, step=2),\n",
    "        \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 0, 20, step=2),\n",
    "        \"gamma\": trial.suggest_int(\"gamma\", 0, 10, step=1),\n",
    "        \"colsample_bytree\": trial.suggest_float(\n",
    "            \"colsample_bytree\", 0.5, 1, step=0.1\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1, step=0.1),\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = XGBClassifier(objective='binary:logistic', **param_grid, early_stopping_rounds=50, eval_metric=\"error\", verbosity=0, n_jobs=4, random_state=42)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = model.predict(X_test)\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"XGB Classifier\")\n",
    "func = lambda trial: xgb_objective(trial, x_train, np.ravel(y_train.values))\n",
    "study.optimize(func, n_trials=200)\n",
    "\n",
    "print(f\"\\tBest value: {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value: 0.66288\n",
    "\tBest params:\n",
    "\t\tn_estimators: 10000\n",
    "\t\tlearning_rate: 0.22707789895194652\n",
    "\t\tmax_depth: 3\n",
    "\t\tmin_child_weight: 2\n",
    "\t\treg_lambda: 14\n",
    "\t\treg_alpha: 8\n",
    "\t\tgamma: 0\n",
    "\t\tcolsample_bytree: 0.9\n",
    "\t\tsubsample: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 30),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 20),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 20),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.6, 1, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 5, 50, step=5),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.6, 1, step=0.1\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1, step=0.1)\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = LGBMClassifier(objective=\"binary\", **param_grid, random_state=42, verbosity=-1, early_stopping_rounds=50)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_error\",\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_test)\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: lgb_objective(trial, x_train, np.ravel(y_train.values))\n",
    "study.optimize(func, n_trials=100)\n",
    "\n",
    "print(f\"\\tBest value: {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value: 0.63010\n",
    "\tBest params:\n",
    "\t\tn_estimators: 10000\n",
    "\t\tlearning_rate: 0.2314223134242193\n",
    "\t\tnum_leaves: 2020\n",
    "\t\tmax_depth: 3\n",
    "\t\tmin_data_in_leaf: 4\n",
    "\t\tlambda_l1: 0\n",
    "\t\tlambda_l2: 5\n",
    "\t\tmin_gain_to_split: 0.6299949073662396\n",
    "\t\tbagging_fraction: 1.0\n",
    "\t\tbagging_freq: 45\n",
    "\t\tfeature_fraction: 0.8\n",
    "\t\tsubsample: 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal number of n_estimators for the chosen hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.6\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "model = XGBClassifier(\n",
    "        objective='binary:logistic', \n",
    "        n_estimators=10000,\n",
    "\t\tlearning_rate=0.22707789895194652,\n",
    "\t\tmax_depth=3,\n",
    "\t\tmin_child_weight=2,\n",
    "\t\treg_lambda=14,\n",
    "\t\treg_alpha=8,\n",
    "\t\tgamma=0,\n",
    "\t\tcolsample_bytree=0.9,\n",
    "\t\tsubsample=0.5,\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric=\"error\",\n",
    "        verbosity=0,\n",
    "        n_jobs=4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "iters = np.empty(5)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "    fx_train, fx_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    fy_train, fy_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        fx_train,\n",
    "        fy_train,\n",
    "        eval_set=[(fx_test, fy_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    iters[idx] = model.best_iteration\n",
    "\n",
    "xgb_iters = np.mean(iters)\n",
    "print(xgb_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6\n"
     ]
    }
   ],
   "source": [
    "#CatBoost\n",
    "model = CatBoostClassifier(\n",
    "        learning_rate= 0.13803526386053072,\n",
    "\t\tmax_depth= 4,\n",
    "\t\tl2_leaf_reg= 19,\n",
    "\t\tcolsample_bylevel= 1.0,\n",
    "\t\tsubsample= 0.7,\n",
    "\t\trandom_strength= 1.0,\n",
    "\t\tbagging_temperature= 4.5,\n",
    "        iterations=10000,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric='Accuracy',\n",
    "        verbose=0\n",
    "        )\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "iters = np.empty(5)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "    fx_train, fx_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    fy_train, fy_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        fx_train,\n",
    "        fy_train,\n",
    "        eval_set=[(fx_test, fy_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    iters[idx] = model.get_best_iteration()\n",
    "\n",
    "cb_iters = np.mean(iters)\n",
    "print(cb_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGB\n",
    "model = LGBMClassifier(\n",
    "        objective=\"binary\", \n",
    "        random_state=42, \n",
    "        verbosity=-1, \n",
    "        early_stopping_rounds=50,\n",
    "        n_estimators=10000,\n",
    "\t\tlearning_rate=0.2314223134242193,\n",
    "\t\tnum_leaves=2020,\n",
    "\t\tmax_depth=3,\n",
    "\t\tmin_data_in_leaf=4,\n",
    "\t\tlambda_l1=0,\n",
    "\t\tlambda_l2=5,\n",
    "\t\tmin_gain_to_split=0.6299949073662396,\n",
    "\t\tbagging_fraction=1.0,\n",
    "\t\tbagging_freq=45,\n",
    "\t\tfeature_fraction=0.8,\n",
    "\t\tsubsample=0.6\n",
    "        )\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "iters = np.empty(5)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "    fx_train, fx_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    fy_train, fy_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        fx_train,\n",
    "        fy_train,\n",
    "        eval_set=[(fx_test, fy_test)],\n",
    "        eval_metric=\"binary_error\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    iters[idx] = model.best_iteration_\n",
    "\n",
    "lgb_iters = np.mean(iters)\n",
    "print(lgb_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(lgb_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        objective='binary:logistic', \n",
    "        n_estimators=34,\n",
    "\t\tlearning_rate=0.22707789895194652,\n",
    "\t\tmax_depth=3,\n",
    "\t\tmin_child_weight=2,\n",
    "\t\treg_lambda=14,\n",
    "\t\treg_alpha=8,\n",
    "\t\tgamma=0,\n",
    "\t\tcolsample_bytree=0.9,\n",
    "\t\tsubsample=0.5,\n",
    "        verbosity=0,\n",
    "        n_jobs=4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "xgb.fit(x_train, y_train, verbose=False)\n",
    "preds = xgb.predict(x_test)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostClassifier(\n",
    "        learning_rate= 0.13803526386053072,\n",
    "\t\tmax_depth= 4,\n",
    "\t\tl2_leaf_reg= 19,\n",
    "\t\tcolsample_bylevel= 1.0,\n",
    "\t\tsubsample= 0.7,\n",
    "\t\trandom_strength= 1.0,\n",
    "\t\tbagging_temperature= 4.5,\n",
    "        iterations=20,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    ")   \n",
    "\n",
    "cb.fit(x_train, y_train, verbose=False)\n",
    "preds = [True if item=='True' else False for item in cb.predict(x_test)]\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(\n",
    "        objective=\"binary\", \n",
    "        random_state=42, \n",
    "        verbosity=-1, \n",
    "        n_estimators=8,\n",
    "\t\tlearning_rate=0.2314223134242193,\n",
    "\t\tnum_leaves=2020,\n",
    "\t\tmax_depth=3,\n",
    "\t\tmin_data_in_leaf=4,\n",
    "\t\tlambda_l1=0,\n",
    "\t\tlambda_l2=5,\n",
    "\t\tmin_gain_to_split=0.6299949073662396,\n",
    "\t\tbagging_fraction=1.0,\n",
    "\t\tbagging_freq=45,\n",
    "\t\tfeature_fraction=0.8,\n",
    "\t\tsubsample=0.6\n",
    ")\n",
    "\n",
    "lgb.fit(x_train, y_train.values.ravel())\n",
    "preds = lgb.predict(x_test)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2. All features + differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_name = [df.columns[i]+'_dif' for i in range(0,len(df.columns[:22]),2)] + [df.columns[i]+'_dif' for i in range(22, 24)] + [df.columns[i]+'_dif' for i in range(25, 33)]\n",
    "\n",
    "for index, name in enumerate(cols_name[:11]):\n",
    "    df[name] = df[df.columns[index*2]] - df[df.columns[index*2+1]]\n",
    "\n",
    "for index, name in enumerate(cols_name[11:13]):\n",
    "    df[name] = df[df.columns[22+index]] - df[df.columns[33+index]]\n",
    "\n",
    "for index, name in enumerate(cols_name[13:]):\n",
    "    df[name] = df[df.columns[25+index]] - df[df.columns[36+index]]\n",
    "\n",
    "df.to_csv('all+diffs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 65)\n",
      "(100, 65)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('all+diffs.csv')\n",
    "\n",
    "Y = dataset.loc[:,'result']\n",
    "X = dataset.drop('result',axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=100, shuffle=True, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf_name</th>\n",
       "      <th>clf_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.591561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.549160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.537640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clf_name  clf_accuracy\n",
       "1  CatBoost      0.591561\n",
       "2  LightGBM      0.549160\n",
       "0   XGBoost      0.537640"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults = pd.DataFrame([['XGBoost',0],['CatBoost',0],['LightGBM',0]],columns=['clf_name', 'clf_accuracy'])\n",
    "models = [XGBClassifier(random_state=random_state), CatBoostClassifier(random_state=random_state, verbose=0), LGBMClassifier(random_state=random_state)]\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    accuracies = []  # List to store the accuracy of each fold\n",
    "    for train_index, test_index in cv.split(x_train):\n",
    "        fx_train, fx_test = x_train.iloc[train_index], x_train.iloc[test_index]\n",
    "        fy_train, fy_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        model.fit(fx_train, fy_train.values.ravel())\n",
    "\n",
    "        preds = model.predict(fx_test)\n",
    "\n",
    "        if index == 1:\n",
    "            preds = [True if item=='True' else False for item in model.predict(fx_test)]\n",
    "\n",
    "        accuracies.append(accuracy_score(fy_test, preds))\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    defaults.loc[index, 'clf_accuracy'] = mean_accuracy\n",
    "\n",
    "defaults.sort_values(by='clf_accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoost\n",
    "def cb_objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 50, step=1),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 0, 20, step=1),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\n",
    "            \"colsample_bylevel\", 0.5, 1, step=0.1\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1, step=0.1),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0, 1, step=0.1),\n",
    "        #\"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\"]),\n",
    "        \"bagging_temperature\": trial.suggest_float('bagging_temperature', 0, 20, step=0.25)\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "                              **param_grid,\n",
    "                              iterations=10000,\n",
    "                              loss_function='Logloss',\n",
    "                              random_seed=42,\n",
    "                              early_stopping_rounds=50,\n",
    "                              eval_metric='Accuracy',\n",
    "                              verbose=0\n",
    "                              )\n",
    " \n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = [True if item=='True' else False for item in model.predict(X_test)]\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"CB Classifier\")\n",
    "func = lambda trial: cb_objective(trial, x_train, np.ravel(y_train.values))\n",
    "study.optimize(func, n_trials=200)\n",
    "\n",
    "print(f\"\\tBest value : {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value : 0.65902\n",
    "\tBest params:\n",
    "\t\tlearning_rate: 0.05351839411162072\n",
    "\t\tmax_depth: 4\n",
    "\t\tmin_data_in_leaf: 31\n",
    "\t\tl2_leaf_reg: 3\n",
    "\t\tcolsample_bylevel: 1.0\n",
    "\t\tsubsample: 0.5\n",
    "\t\trandom_strength: 0.8\n",
    "\t\tbagging_temperature: 10.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "def xgb_objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"eta\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0, 10, step=1),\n",
    "        \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 0, 20, step=2),\n",
    "        \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 0, 20, step=2),\n",
    "        \"gamma\": trial.suggest_int(\"gamma\", 0, 10, step=1),\n",
    "        \"colsample_bytree\": trial.suggest_float(\n",
    "            \"colsample_bytree\", 0.5, 1, step=0.1\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1, step=0.1),\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = XGBClassifier(objective='binary:logistic', **param_grid, early_stopping_rounds=50, eval_metric=\"error\", verbosity=0, n_jobs=4, random_state=42)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = model.predict(X_test)\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"XGB Classifier\")\n",
    "func = lambda trial: xgb_objective(trial, x_train, np.ravel(y_train.values))\n",
    "study.optimize(func, n_trials=200)\n",
    "\n",
    "print(f\"\\tBest value: {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value: 0.64554\n",
    "\tBest params:\n",
    "\t\tn_estimators: 10000\n",
    "\t\tlearning_rate: 0.060726041010120094\n",
    "\t\tmax_depth: 4\n",
    "\t\tmin_child_weight: 9\n",
    "\t\treg_lambda: 2\n",
    "\t\treg_alpha: 0\n",
    "\t\tgamma: 2\n",
    "\t\tcolsample_bytree: 0.6\n",
    "\t\tsubsample: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBClassifier\n",
    "def lgb_objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 30),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 20),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 20),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.6, 1, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 5, 50, step=5),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.6, 1, step=0.1\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1, step=0.1)\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = LGBMClassifier(objective=\"binary\", **param_grid, random_state=42, verbosity=-1, early_stopping_rounds=50)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_error\",\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_test)\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: lgb_objective(trial, x_train, np.ravel(y_train.values))\n",
    "study.optimize(func, n_trials=200)\n",
    "\n",
    "print(f\"\\tBest value: {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value: 0.62431\n",
    "\tBest params:\n",
    "\t\tn_estimators: 10000\n",
    "\t\tlearning_rate: 0.09554692701030004\n",
    "\t\tnum_leaves: 2860\n",
    "\t\tmax_depth: 3\n",
    "\t\tmin_data_in_leaf: 9\n",
    "\t\tlambda_l1: 0\n",
    "\t\tlambda_l2: 18\n",
    "\t\tmin_gain_to_split: 4.111913891598791\n",
    "\t\tbagging_fraction: 1.0\n",
    "\t\tbagging_freq: 15\n",
    "\t\tfeature_fraction: 0.6\n",
    "\t\tsubsample: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.4\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "model = XGBClassifier(\n",
    "        objective='binary:logistic', \n",
    "        n_estimators=10000,\n",
    "\t\tlearning_rate=0.060726041010120094,\n",
    "\t\tmax_depth=4,\n",
    "\t\tmin_child_weight=9,\n",
    "\t\treg_lambda=2,\n",
    "\t\treg_alpha=0,\n",
    "\t\tgamma=2,\n",
    "\t\tcolsample_bytree=0.6,\n",
    "\t\tsubsample=0.5,\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric=\"error\",\n",
    "        verbosity=0,\n",
    "        n_jobs=4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "iters = np.empty(5)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "    fx_train, fx_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    fy_train, fy_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        fx_train,\n",
    "        fy_train,\n",
    "        eval_set=[(fx_test, fy_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    iters[idx] = model.best_iteration\n",
    "\n",
    "xgb_iters = np.mean(iters)\n",
    "print(xgb_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "#CatBoost\n",
    "model = CatBoostClassifier(\n",
    "        learning_rate= 0.05351839411162072,\n",
    "\t\tmax_depth= 4,\n",
    "        min_data_in_leaf= 31,\n",
    "\t\tl2_leaf_reg= 3,\n",
    "\t\tcolsample_bylevel= 1.0,\n",
    "\t\tsubsample= 0.5,\n",
    "\t\trandom_strength= 0.8,\n",
    "\t\tbagging_temperature= 10.25,\n",
    "        iterations=10000,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric='Accuracy',\n",
    "        verbose=0\n",
    "        )\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "iters = np.empty(5)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "    fx_train, fx_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    fy_train, fy_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        fx_train,\n",
    "        fy_train,\n",
    "        eval_set=[(fx_test, fy_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    iters[idx] = model.get_best_iteration()\n",
    "\n",
    "cb_iters = np.mean(iters)\n",
    "print(cb_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGB\n",
    "model = LGBMClassifier(\n",
    "        objective=\"binary\", \n",
    "        random_state=42, \n",
    "        verbosity=-1, \n",
    "        early_stopping_rounds=50,\n",
    "        n_estimators=10000,\n",
    "\t\tlearning_rate=0.09554692701030004,\n",
    "\t\tnum_leaves=2860,\n",
    "\t\tmax_depth=3,\n",
    "\t\tmin_data_in_leaf=9,\n",
    "\t\tlambda_l1=0,\n",
    "\t\tlambda_l2=18,\n",
    "\t\tmin_gain_to_split=4.111913891598791,\n",
    "\t\tbagging_fraction=1.0,\n",
    "\t\tbagging_freq=15,\n",
    "\t\tfeature_fraction=0.6,\n",
    "\t\tsubsample=1.0\n",
    "        )\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "iters = np.empty(5)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "    fx_train, fx_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    fy_train, fy_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        fx_train,\n",
    "        fy_train,\n",
    "        eval_set=[(fx_test, fy_test)],\n",
    "        eval_metric=\"binary_error\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    iters[idx] = model.best_iteration_\n",
    "\n",
    "lgb_iters = np.mean(iters)\n",
    "print(lgb_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        objective='binary:logistic', \n",
    "\t\tlearning_rate=0.060726041010120094,\n",
    "\t\tmax_depth=4,\n",
    "\t\tmin_child_weight=9,\n",
    "\t\treg_lambda=2,\n",
    "\t\treg_alpha=0,\n",
    "\t\tgamma=2,\n",
    "\t\tcolsample_bytree=0.6,\n",
    "\t\tsubsample=0.5,\n",
    "        verbosity=0,\n",
    "        n_jobs=4,\n",
    "        random_state=42,\n",
    "        n_estimators=28\n",
    "    )\n",
    "\n",
    "xgb.fit(x_train, y_train, verbose=False)\n",
    "preds = xgb.predict(x_test)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostClassifier(\n",
    "        learning_rate= 0.05351839411162072,\n",
    "\t\tmax_depth= 4,\n",
    "        min_data_in_leaf= 31,\n",
    "\t\tl2_leaf_reg= 3,\n",
    "\t\tcolsample_bylevel= 1.0,\n",
    "\t\tsubsample= 0.5,\n",
    "\t\trandom_strength= 0.8,\n",
    "\t\tbagging_temperature= 10.25,\n",
    "        iterations=9,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    ")   \n",
    "\n",
    "cb.fit(x_train, y_train, verbose=False)\n",
    "preds = [True if item=='True' else False for item in cb.predict(x_test)]\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(\n",
    "        objective=\"binary\", \n",
    "        random_state=42, \n",
    "        verbosity=-1, \n",
    "        n_estimators=6,\n",
    "\t\tlearning_rate=0.09554692701030004,\n",
    "\t\tnum_leaves=2860,\n",
    "\t\tmax_depth=3,\n",
    "\t\tmin_data_in_leaf=9,\n",
    "\t\tlambda_l1=0,\n",
    "\t\tlambda_l2=18,\n",
    "\t\tmin_gain_to_split=4.111913891598791,\n",
    "\t\tbagging_fraction=1.0,\n",
    "\t\tbagging_freq=15,\n",
    "\t\tfeature_fraction=0.6,\n",
    "\t\tsubsample=1.0\n",
    ")\n",
    "\n",
    "lgb.fit(x_train, y_train.values.ravel())\n",
    "preds = lgb.predict(x_test)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3. Different Data split (Same 100 for test, 10 fold -> 50 for validation) --> No particular effect (Deleted code cells since mainly was just copypasting from previous experiments with minor code changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4. Tried to change the eval_metric to logloss --> No particular effect (Deleted code cells since mainly was just copypasting from previous experiments with minor code changes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
