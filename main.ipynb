{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query data from PostgreSQL table, save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\", dbname='cs', user='postgres', password='123456', port=5432)\n",
    "cur = conn.cursor()\n",
    "\n",
    "query = \"\"\"SELECT * FROM side_dataset where id>=684\"\"\"\n",
    "cur.execute(query)\n",
    "\n",
    "rows = cur.fetchall()\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "df.to_csv('df_642.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('df_642.csv')\n",
    "#First 4 rows are not needed\n",
    "df = data.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While parsing the data, I was storing this particular stat (average rounds lost(won) on a map) as 0's in case of missing data. So, I convert them to NULL, to deal with them further. Also deelting the rows with incorrect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['t1_rounds_lost']>0) & (df['t1_rounds_won'] == 0), 't1_rounds_won'] = None\n",
    "df.loc[(df['t1_rounds_lost']==0) & (df['t1_rounds_won'] > 0), 't1_rounds_lost'] = None\n",
    "df.loc[(df['t2_rounds_lost']==0) & (df['t2_rounds_won'] > 0), 't2_rounds_lost'] = None\n",
    "df.loc[(df['t2_rounds_lost']>0) & (df['t2_rounds_won'] == 0), 't2_rounds_won'] = None\n",
    "\n",
    "df.drop(df[(df['t2_5v4']==0) & (df['t2_played']>0)].index, axis=0, inplace=True)\n",
    "df.drop(df[(df['t1_5v4']==0) & (df['t1_played']>0)].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try filling NULL values with 13's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t1_rounds_lost'].fillna(13, inplace=True)\n",
    "df['t1_rounds_won'].fillna(13, inplace=True)\n",
    "df['t2_rounds_lost'].fillna(13, inplace=True)\n",
    "df['t2_rounds_won'].fillna(13, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The differences between team stats will be the predicting features, so creating the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_name = [df.columns[i]+'_dif' for i in range(0,len(df.columns[:22]),2)] + [df.columns[i]+'_dif' for i in range(22, 24)] + [df.columns[i]+'_dif' for i in range(25, 33)]\n",
    "\n",
    "dataset = pd.DataFrame(columns=cols_name)\n",
    "\n",
    "dataset = pd.DataFrame(columns=cols_name)\n",
    "for index, name in enumerate(cols_name[:11]):\n",
    "    dataset[name] = df[df.columns[index*2]] - df[df.columns[index*2+1]]\n",
    "\n",
    "for index, name in enumerate(cols_name[11:13]):\n",
    "    dataset[name] = df[df.columns[22+index]] - df[df.columns[33+index]]\n",
    "\n",
    "for index, name in enumerate(cols_name[13:]):\n",
    "    dataset[name] = df[df.columns[25+index]] - df[df.columns[36+index]]\n",
    "\n",
    "dataset['t1_fp'] = df['t1_fp']\n",
    "dataset['t2_fp'] = df['t2_fp']\n",
    "dataset['result'] = df['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "columns = ['t1_winstreak_dif','t1_h2h_dif','t1_ranking_dif','t1_pluses_dif','t1_minuses_dif','t1_coef_dif','t1_rating_dif','t1_event_rating_dif', \\\n",
    "            't1_num_maps_dif','t1_avg_lost_dif','t1_avg_won_dif','t1_rounds_lost_dif','t1_rounds_won_dif','t1_fp_percent_dif','t1_winrate_dif', \\\n",
    "            't1_played_dif','t1_map_winstreak_dif','t1_map_losestreak_dif','t1_5v4_dif','t1_4v5_dif','t1_pistol_dif']\n",
    "\n",
    "scaler.fit(dataset.loc[:,columns])\n",
    "dataset.loc[:,columns] = scaler.transform(dataset.loc[:,columns])\n",
    "\n",
    "with open('scaler.pkl','wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(419, 23)\n",
      "(100, 23)\n",
      "(100, 23)\n"
     ]
    }
   ],
   "source": [
    "Y = dataset.loc[:,['result']]\n",
    "X = dataset.drop('result',axis=1)\n",
    "\n",
    "x_train, x_rest, y_train, y_rest = train_test_split(X, Y, test_size=200, shuffle=True, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_rest, y_rest, test_size=100, shuffle=True, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
